# -*- coding: utf-8 -*-
"""Copy of llamatest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G0V3JFArcRMCJzuO_tWUTzOn5SvL97gE
"""

import os
import json
from huggingface_hub import InferenceClient

# Initialize Hugging Face Inference Client
client = InferenceClient(api_key=p"")  # Replace with your Hugging Face API key

# Define exact prompts
prompts = {
    "prompt1": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt2": """Analyze the provided ECG image step by step to determine if it represents an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt3": """Evaluate the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Confidence Level": "[High/Medium/Low]",
        "Rhythm Regularity": {
            "Observation": "[Regular/Irregularly Irregular/Regularly Irregular]",
            "Confidence": "[High/Medium/Low]"
        },
        "P Waves": {
            "Observation": "[Present/Absent/Indiscernible]",
            "Confidence": "[High/Medium/Low]"
        },
        "Baseline Activity": {
            "Observation": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
            "Confidence": "[High/Medium/Low]"
        },
        "Ventricular Rate": {
            "Observation": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
            "Confidence": "[High/Medium/Low]"
        },
        "QRS Complex Morphology": {
            "Observation": "[Constant/Variable]",
            "Confidence": "[High/Medium/Low]"
        },
        "PR Interval": {
            "Observation": "[Normal/Variable/Prolonged/Short/Not Measurable]",
            "Confidence": "[High/Medium/Low]"
        }
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """
}

# Path to the folder containing the images and the text file with links
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/testgpt'  # Replace with your folder path
links_file = os.path.join(folder_path, 'Public_Image_Links_With_Names.txt')  # Replace with the file name containing image links

# Read the links file
with open(links_file, 'r') as f:
    image_links = [line.strip().split(': ') for line in f.readlines()]

# Loop through each image and prompt
for image_name, image_link in image_links:
    for prompt_name, prompt in prompts.items():
        # Prepare the message for Llama Vision
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_link
                        }
                    }
                ]
            }
        ]

        # Send the request to Llama Vision
        try:
            completion = client.chat.completions.create(
                model="meta-llama/Llama-3.2-11B-Vision-Instruct",
                messages=messages,
                max_tokens=500
            )

            # Extract the response
            result = completion.choices[0].message["content"]

            # Save the response in a JSON file
            json_filename = os.path.join(folder_path, f"{image_name}_llama_{prompt_name}.json")
            with open(json_filename, 'w') as json_file:
                json.dump(result, json_file, indent=4)
        except Exception as e:
            print(f"Error processing {image_name} with {prompt_name}: {e}")

import os
import json

# Path to the folder containing the JSON files
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/testgpt'  # Replace with your folder path

# Function to clean and format the JSON content
def clean_json_file(file_path):
    try:
        # Read the raw content from the JSON file
        with open(file_path, 'r') as f:
            raw_content = f.read()

        # Parse the JSON content
        parsed_content = json.loads(raw_content)

        # Reformat the JSON content to remove unnecessary characters
        clean_content = json.dumps(parsed_content, indent=4)

        # Overwrite the file with the cleaned content
        with open(file_path, 'w') as f:
            f.write(clean_content)

        print(f"Cleaned and reformatted: {file_path}")
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON in file {file_path}: {e}")
    except Exception as e:
        print(f"Error processing file {file_path}: {e}")

# Iterate through all JSON files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.json'):  # Process only JSON files
        file_path = os.path.join(folder_path, filename)
        clean_json_file(file_path)

import os
import json
from huggingface_hub import InferenceClient

# Initialize Hugging Face Inference Client
client = InferenceClient(api_key="")  # Replace with your Hugging Face API key

# Define exact prompts
prompts = {
    "prompt1": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt2": """Analyze the provided ECG image step by step to determine if it represents an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt3": """Evaluate the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Confidence Level": "[High/Medium/Low]",
        "Rhythm Regularity": {
            "Observation": "[Regular/Irregularly Irregular/Regularly Irregular]",
            "Confidence": "[High/Medium/Low]"
        },
        "P Waves": {
            "Observation": "[Present/Absent/Indiscernible]",
            "Confidence": "[High/Medium/Low]"
        },
        "Baseline Activity": {
            "Observation": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
            "Confidence": "[High/Medium/Low]"
        },
        "Ventricular Rate": {
            "Observation": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
            "Confidence": "[High/Medium/Low]"
        },
        "QRS Complex Morphology": {
            "Observation": "[Constant/Variable]",
            "Confidence": "[High/Medium/Low]"
        },
        "PR Interval": {
            "Observation": "[Normal/Variable/Prolonged/Short/Not Measurable]",
            "Confidence": "[High/Medium/Low]"
        }
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """
}

# Path to the folder containing the images and the text file with links
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/test2'  # Replace with your folder path
links_file = os.path.join(folder_path, 'Public_Image_Links_With_Names.txt')  # Replace with the file name containing image links

# Read the links file
with open(links_file, 'r') as f:
    image_links = [line.strip().split(': ') for line in f.readlines()]

# Function to ensure JSON format is correct and well-structured
def format_to_requested_structure(raw_response, prompt_name):
    try:
        # Parse the response
        parsed_json = json.loads(raw_response)

        # Define the required structure based on the prompt
        if "prompt1" in prompt_name or "prompt2" in prompt_name:
            required_keys = [
                "Diagnosis", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval"
            ]
        else:
            required_keys = [
                "Diagnosis", "Confidence Level", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval"
            ]

        # Ensure all required keys exist
        cleaned_json = {key: parsed_json.get(key, "Missing Value") for key in required_keys}

        # Reformat to proper JSON string
        return json.dumps(cleaned_json, indent=4)
    except json.JSONDecodeError:
        print("Invalid JSON format received. Attempting to restructure.")
        return json.dumps({"Error": "Response could not be formatted as JSON."}, indent=4)

# Loop through each image and prompt
for image_name, image_link in image_links:
    for prompt_name, prompt in prompts.items():
        # Prepare the message for Llama Vision
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_link
                        }
                    }
                ]
            }
        ]

        # Send the request to Llama Vision
        try:
            completion = client.chat.completions.create(
                model="meta-llama/Llama-3.2-11B-Vision-Instruct",
                messages=messages,
                max_tokens=500
            )

            # Extract the raw response
            raw_result = completion.choices[0].message["content"]

            # Format the response into the requested structure
            formatted_result = format_to_requested_structure(raw_result, prompt_name)

            # Save the formatted response in a JSON file
            json_filename = os.path.join(folder_path, f"{image_name}_llama_{prompt_name}.json")
            with open(json_filename, 'w') as json_file:
                json_file.write(formatted_result)
        except Exception as e:
            print(f"Error processing {image_name} with {prompt_name}: {e}")

import os
import json
from huggingface_hub import InferenceClient

# Initialize Hugging Face Inference Client
client = InferenceClient(api_key="")  # Replace with your Hugging Face API key

# Define exact prompts
prompts = {
    "prompt1": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt2": """Analyze the provided ECG image step by step to determine if it represents an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt3": """Evaluate the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Confidence Level": "[High/Medium/Low]",
        "Rhythm Regularity": {
            "Observation": "[Regular/Irregularly Irregular/Regularly Irregular]",
            "Confidence": "[High/Medium/Low]"
        },
        "P Waves": {
            "Observation": "[Present/Absent/Indiscernible]",
            "Confidence": "[High/Medium/Low]"
        },
        "Baseline Activity": {
            "Observation": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
            "Confidence": "[High/Medium/Low]"
        },
        "Ventricular Rate": {
            "Observation": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
            "Confidence": "[High/Medium/Low]"
        },
        "QRS Complex Morphology": {
            "Observation": "[Constant/Variable]",
            "Confidence": "[High/Medium/Low]"
        },
        "PR Interval": {
            "Observation": "[Normal/Variable/Prolonged/Short/Not Measurable]",
            "Confidence": "[High/Medium/Low]"
        }
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """
}

# Path to the folder containing the images and the text file with links
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/test2'  # Replace with your folder path
links_file = os.path.join(folder_path, 'Public_Image_Links_With_Names.txt')  # Replace with the file name containing image links

# Read the links file
with open(links_file, 'r') as f:
    image_links = [line.strip().split(': ') for line in f.readlines()]

# Function to ensure JSON format is correct and well-structured
def format_to_requested_structure(raw_response, prompt_name):
    try:
        # Parse the response
        parsed_json = json.loads(raw_response)

        # Define the required structure based on the prompt
        if "prompt1" in prompt_name or "prompt2" in prompt_name:
            required_keys = [
                "Diagnosis", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval"
            ]
        else:
            required_keys = [
                "Diagnosis", "Confidence Level", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval"
            ]

        # Ensure all required keys exist
        cleaned_json = {key: parsed_json.get(key, "Missing Value") for key in required_keys}

        # Return the cleaned JSON object
        return cleaned_json, True
    except json.JSONDecodeError:
        # Return raw response wrapped in a JSON object
        return {"raw_response": raw_response}, False

# Loop through each image and prompt
for image_name, image_link in image_links:
    for prompt_name, prompt in prompts.items():
        # Prepare the message for Llama Vision
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_link
                        }
                    }
                ]
            }
        ]

        # Send the request to Llama Vision
        try:
            completion = client.chat.completions.create(
                model="meta-llama/Llama-3.2-11B-Vision-Instruct",
                messages=messages,
                max_tokens=500
            )

            # Extract the raw response
            raw_result = completion.choices[0].message["content"]

            # Format the response into the requested structure
            formatted_result, is_json = format_to_requested_structure(raw_result, prompt_name)

            # Save the response in a JSON file
            json_filename = os.path.join(folder_path, f"{image_name}_llama_{prompt_name}.json")
            with open(json_filename, 'w') as json_file:
                json.dump(formatted_result, json_file, indent=4)
        except Exception as e:
            print(f"Error processing {image_name} with {prompt_name}: {e}")

import os
import json
from huggingface_hub import InferenceClient

# Initialize Hugging Face Inference Client
client = InferenceClient(api_key="")  # Replace with your Hugging Face API key

# Define exact prompts
prompts = {
    "prompt1": """ <image> <bos> As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[Sinus Rhythm/Not Sinus Rhythm]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable] at [number] milliseconds",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable] at [number] milliseconds"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt2": """Analyze the provided ECG image step by step to determine if it represents an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[Sinus Rhythm/Not Sinus Rhythm]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable] at [number] milliseconds",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable] at [number] milliseconds"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt3": """Evaluate the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[Sinus Rhythm/Not Sinus Rhythm]",
        "Confidence Level": "[High/Medium/Low]",
        "Rhythm Regularity": {
            "Observation": "[Regular/Irregularly Irregular/Regularly Irregular]",
            "Confidence": "[High/Medium/Low]"
        },
        "P Waves": {
            "Observation": "[Present/Absent/Indiscernible]",
            "Confidence": "[High/Medium/Low]"
        },
        "Baseline Activity": {
            "Observation": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
            "Confidence": "[High/Medium/Low]"
        },
        "Ventricular Rate": {
            "Observation": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
            "Confidence": "[High/Medium/Low]"
        },
        "QRS Complex Morphology": {
            "Observation": "[Constant/Variable] at [number] milliseconds",
            "Confidence": "[High/Medium/Low]"
        },
        "PR Interval": {
            "Observation": "[Normal/Variable/Prolonged/Short/Not Measurable] at [number] milliseconds",
            "Confidence": "[High/Medium/Low]"
        }
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """
}

# Path to the folder containing the images and the text file with links
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/Sinus Rhythm'  # Replace with your folder path
links_file = os.path.join(folder_path, 'Public_Image_Links_With_Names.txt')  # Replace with the file name containing image links

# Read the links file
with open(links_file, 'r') as f:
    image_links = [line.strip().split(': ') for line in f.readlines()]

# Function to ensure JSON format is correct and well-structured
def format_to_requested_structure(raw_response, prompt_name):
    try:
        # Check if response contains valid JSON as a string
        if "raw_response" in raw_response:
            inner_json = json.loads(raw_response["raw_response"])
            return inner_json, True
        # Parse the response if it's already JSON
        parsed_json = json.loads(raw_response)

        # Define the required structure based on the prompt
        if "prompt1" in prompt_name or "prompt2" in prompt_name:
            required_keys = [
                "Diagnosis", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval"
            ]
        else:
            required_keys = [
                "Diagnosis", "Confidence Level", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval"
            ]

        # Ensure all required keys exist
        cleaned_json = {key: parsed_json.get(key, "Missing Value") for key in required_keys}
        return cleaned_json, True
    except json.JSONDecodeError:
        # Handle cases where the raw response is JSON-like but escaped
        try:
            parsed_json = json.loads(raw_response.strip('"'))  # Remove enclosing quotes
            return format_to_requested_structure(json.dumps(parsed_json), prompt_name)  # Retry formatting
        except:
            # Return raw response if it can't be processed
            return {"raw_response": raw_response}, False

# Loop through each image and prompt
for image_name, image_link in image_links:
    for prompt_name, prompt in prompts.items():
        # Prepare the message for Llama Vision
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_link
                        }
                    }
                ]
            }
        ]

        # Send the request to Llama Vision
        try:
            completion = client.chat.completions.create(
                model="meta-llama/Llama-3.2-11B-Vision-Instruct",
                messages=messages,
                max_tokens=500
            )

            # Extract the raw response
            raw_result = completion.choices[0].message["content"]

            # Format the response into the requested structure
            formatted_result, is_json = format_to_requested_structure(raw_result, prompt_name)

            # Save the response in a JSON file
            json_filename = os.path.join(folder_path, f"{image_name}_llama_{prompt_name}.json")
            with open(json_filename, 'w') as json_file:
                json.dump(formatted_result, json_file, indent=4)
        except Exception as e:
            print(f"Error processing {image_name} with {prompt_name}: {e}")

import os
import json
from collections import defaultdict
import pandas as pd

# Update the path to the folder containing the JSON files
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/Sinus Rhythm'

# Ground truth definitions for Sinus Rhythm and AFIB
expected_responses = {
    "Sinus Rhythm": {
        "Diagnosis": "Sinus Rhythm",
        "Rhythm Regularity": "Regular",
        "P Waves": "Present",
        "Baseline Activity": "Normal",
        "Ventricular Rate": "Normal",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Normal"
    },
    "AFIB": {
        "Diagnosis": "AFIB",
        "Rhythm Regularity": "Irregularly Irregular",
        "P Waves": "Absent",
        "Baseline Activity": "Fibrillatory Waves",
        "Ventricular Rate": "Tachycardic",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Not Measurable"
    }
}

ground_truth_key = "Sinus Rhythm"
ground_truth = expected_responses[ground_truth_key]

statistics = defaultdict(lambda: defaultdict(int))
feature_stats = defaultdict(lambda: defaultdict(list))

def evaluate_response(json_data, ground_truth, features):
    comparison = {}
    for feature in features:
        comparison[feature] = json_data.get(feature, "Missing Value") == ground_truth.get(feature, "Missing Value")
    return comparison

valid_responses = 0
total_responses = 0
results = []

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        total_responses += 1
        file_path = os.path.join(folder_path, filename)

        with open(file_path, 'r') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue

            if isinstance(data, dict) and "Diagnosis" in data:
                valid_responses += 1
                comparison = evaluate_response(data, ground_truth, ground_truth.keys())

                for feature, is_correct in comparison.items():
                    statistics[feature]["correct"] += is_correct
                    statistics[feature]["total"] += 1
                    feature_stats[feature][filename].append(is_correct)

                # Extract prompt name more precisely based on suffix
                prompt = "Prompt 1" if "llama_prompt1" in filename else \
                         "Prompt 2" if "llama_prompt2" in filename else \
                         "Prompt 3" if "llama_prompt3" in filename else "Unknown"

                results.append({
                    "Filename": filename,
                    "Prompt": prompt,
                    **comparison
                })

for feature, counts in statistics.items():
    counts["accuracy"] = counts["correct"] / counts["total"] * 100 if counts["total"] > 0 else 0

df = pd.DataFrame(results)
prompt_stats = df.groupby("Prompt").mean(numeric_only=True).reset_index()

output_folder = os.path.join(folder_path, f"{ground_truth_key}_Analysis")
os.makedirs(output_folder, exist_ok=True)

df.to_csv(os.path.join(output_folder, f"detailed_results_{ground_truth_key}.csv"), index=False)
prompt_stats.to_csv(os.path.join(output_folder, f"prompt_summary_{ground_truth_key}.csv"), index=False)

print(f"Analysis for {ground_truth_key} Cases")
print("Feature-level Statistics:")
for feature, counts in statistics.items():
    print(f"{feature}: {counts['accuracy']:.2f}% accuracy")

print("\nPrompt-level Statistics:")
print(prompt_stats)



import os
import json
from collections import defaultdict
import pandas as pd

# Update the path to the folder containing the JSON files
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/A.Fib'

# Ground truth definitions for Sinus Rhythm and AFIB
expected_responses = {
    "Sinus Rhythm": {
        "Diagnosis": "Sinus Rhythm",
        "Rhythm Regularity": "Regular",
        "P Waves": "Present",
        "Baseline Activity": "Normal",
        "Ventricular Rate": "Normal",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Normal"
    },
    "AFIB": {
        "Diagnosis": "AFIB",
        "Rhythm Regularity": "Irregularly Irregular",
        "P Waves": "Absent",
        "Baseline Activity": "Fibrillatory Waves",
        "Ventricular Rate": "Tachycardic",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Not Measurable"
    }
}

ground_truth_key = "AFIB"
ground_truth = expected_responses[ground_truth_key]

statistics = defaultdict(lambda: defaultdict(int))
feature_stats = defaultdict(lambda: defaultdict(list))

def evaluate_response(json_data, ground_truth, features):
    comparison = {}
    for feature in features:
        comparison[feature] = json_data.get(feature, "Missing Value") == ground_truth.get(feature, "Missing Value")
    return comparison

valid_responses = 0
total_responses = 0
results = []

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        total_responses += 1
        file_path = os.path.join(folder_path, filename)

        with open(file_path, 'r') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue

            if isinstance(data, dict) and "Diagnosis" in data:
                valid_responses += 1
                comparison = evaluate_response(data, ground_truth, ground_truth.keys())

                for feature, is_correct in comparison.items():
                    statistics[feature]["correct"] += is_correct
                    statistics[feature]["total"] += 1
                    feature_stats[feature][filename].append(is_correct)

                # Extract prompt name more precisely based on suffix
                prompt = "Prompt 1" if "llama_prompt1" in filename else \
                         "Prompt 2" if "llama_prompt2" in filename else \
                         "Prompt 3" if "llama_prompt3" in filename else "Unknown"

                results.append({
                    "Filename": filename,
                    "Prompt": prompt,
                    **comparison
                })

for feature, counts in statistics.items():
    counts["accuracy"] = counts["correct"] / counts["total"] * 100 if counts["total"] > 0 else 0

df = pd.DataFrame(results)
prompt_stats = df.groupby("Prompt").mean(numeric_only=True).reset_index()

output_folder = os.path.join(folder_path, f"{ground_truth_key}_Analysis")
os.makedirs(output_folder, exist_ok=True)

df.to_csv(os.path.join(output_folder, f"detailed_results_{ground_truth_key}.csv"), index=False)
prompt_stats.to_csv(os.path.join(output_folder, f"prompt_summary_{ground_truth_key}.csv"), index=False)

print(f"Analysis for {ground_truth_key} Cases")
print("Feature-level Statistics:")
for feature, counts in statistics.items():
    print(f"{feature}: {counts['accuracy']:.2f}% accuracy")

print("\nPrompt-level Statistics:")
print(prompt_stats)

import os
import json
from collections import defaultdict
import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Update the path to the folder containing the JSON files
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/A.Fib'

# Ground truth definitions for Sinus Rhythm and AFIB
expected_responses = {
    "Sinus Rhythm": {
        "Diagnosis": "Sinus Rhythm",
        "Rhythm Regularity": "Regular",
        "P Waves": "Present",
        "Baseline Activity": "Normal",
        "Ventricular Rate": "Normal",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Normal"
    },
    "AFIB": {
        "Diagnosis": "AFIB",
        "Rhythm Regularity": "Irregularly Irregular",
        "P Waves": "Absent",
        "Baseline Activity": "Fibrillatory Waves",
        "Ventricular Rate": "Tachycardic",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Not Measurable"
    }
}

ground_truth_key = "AFIB"
ground_truth = expected_responses[ground_truth_key]

statistics = defaultdict(lambda: defaultdict(int))
feature_stats = defaultdict(lambda: defaultdict(list))
precision_scores = defaultdict(list)
recall_scores = defaultdict(list)
f1_scores = defaultdict(list)
error_rates = defaultdict(float)
confidence_intervals = defaultdict(lambda: (0, 0))

# Helper function to calculate confidence intervals
def calculate_confidence_interval(accuracy, total):
    if total > 0:
        z = 1.96  # 95% confidence interval
        p = accuracy / 100
        margin = z * np.sqrt((p * (1 - p)) / total)
        return max(0, p - margin) * 100, min(100, p + margin) * 100
    return 0, 0

def evaluate_response(json_data, ground_truth, features):
    comparison = {}
    for feature in features:
        comparison[feature] = json_data.get(feature, "Missing Value") == ground_truth.get(feature, "Missing Value")
    return comparison

valid_responses = 0
total_responses = 0
results = []

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        total_responses += 1
        file_path = os.path.join(folder_path, filename)

        with open(file_path, 'r') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue

            if isinstance(data, dict) and "Diagnosis" in data:
                valid_responses += 1
                comparison = evaluate_response(data, ground_truth, ground_truth.keys())

                for feature, is_correct in comparison.items():
                    statistics[feature]["correct"] += is_correct
                    statistics[feature]["total"] += 1
                    feature_stats[feature][filename].append(is_correct)

                # Extract prompt name more precisely based on suffix
                prompt = "Prompt 1" if "llama_prompt1" in filename else \
                         "Prompt 2" if "llama_prompt2" in filename else \
                         "Prompt 3" if "llama_prompt3" in filename else "Unknown"

                results.append({
                    "Filename": filename,
                    "Prompt": prompt,
                    **comparison
                })

# Calculate additional metrics
for feature, counts in statistics.items():
    accuracy = counts["correct"] / counts["total"] * 100 if counts["total"] > 0 else 0
    error_rates[feature] = 100 - accuracy
    confidence_intervals[feature] = calculate_confidence_interval(accuracy, counts["total"])
    counts["accuracy"] = accuracy

df = pd.DataFrame(results)
prompt_stats = df.groupby("Prompt").mean(numeric_only=True).reset_index()

# Compute precision, recall, and F1-score for each feature
for feature in ground_truth.keys():
    y_true = [ground_truth[feature]] * len(df)
    y_pred = df[feature].apply(lambda x: ground_truth[feature] if x else "Incorrect").tolist()
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])

    precision_scores[feature].append(precision)
    recall_scores[feature].append(recall)
    f1_scores[feature].append(f1)

# Save results to CSV
output_folder = os.path.join(folder_path, f"{ground_truth_key}_Analysis")
os.makedirs(output_folder, exist_ok=True)

df.to_csv(os.path.join(output_folder, f"detailed_results_{ground_truth_key}.csv"), index=False)
prompt_stats.to_csv(os.path.join(output_folder, f"prompt_summary_{ground_truth_key}.csv"), index=False)

# Visualization
# Feature-level accuracy
features = list(statistics.keys())
accuracies = [statistics[feature]["accuracy"] for feature in features]
error_rates_list = [error_rates[feature] for feature in features]

plt.figure(figsize=(10, 6))
plt.bar(features, accuracies, label="Accuracy (%)", alpha=0.7)
plt.bar(features, error_rates_list, label="Error Rate (%)", alpha=0.7)
plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Feature-Level Accuracy and Error Rate")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "feature_accuracy_error_rate.png"))
plt.show()

# Precision, Recall, and F1-score per feature
precision_vals = [precision_scores[feature][0] * 100 for feature in features]
recall_vals = [recall_scores[feature][0] * 100 for feature in features]
f1_vals = [f1_scores[feature][0] * 100 for feature in features]

plt.figure(figsize=(10, 6))
bar_width = 0.25
x_indices = np.arange(len(features))

plt.bar(x_indices, precision_vals, bar_width, label="Precision (%)")
plt.bar(x_indices + bar_width, recall_vals, bar_width, label="Recall (%)")
plt.bar(x_indices + 2 * bar_width, f1_vals, bar_width, label="F1-Score (%)")

plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Precision, Recall, and F1-Score by Feature")
plt.xticks(x_indices + bar_width, features, rotation=45)
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "precision_recall_f1.png"))
plt.show()

# Print summary
print(f"Analysis for {ground_truth_key} Cases")
print("Feature-level Statistics:")
for feature, counts in statistics.items():
    print(f"{feature}: {counts['accuracy']:.2f}% accuracy, Error Rate: {error_rates[feature]:.2f}%, Confidence Interval: {confidence_intervals[feature]}")

print("\nPrecision, Recall, and F1-Score:")
for feature in ground_truth.keys():
    print(f"{feature}: Precision: {precision_scores[feature][0]:.2f}, Recall: {recall_scores[feature][0]:.2f}, F1-Score: {f1_scores[feature][0]:.2f}")

print("\nPrompt-level Statistics:")
print(prompt_stats)

import os
import json
from collections import defaultdict
import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Update the path to the folder containing the JSON files
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/Sinus Rhythm'

# Ground truth definitions for Sinus Rhythm and AFIB
expected_responses = {
    "Sinus Rhythm": {
        "Diagnosis": "Sinus Rhythm",
        "Rhythm Regularity": "Regular",
        "P Waves": "Present",
        "Baseline Activity": "Normal",
        "Ventricular Rate": "Normal",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Normal"
    },
    "AFIB": {
        "Diagnosis": "AFIB",
        "Rhythm Regularity": "Irregularly Irregular",
        "P Waves": "Absent",
        "Baseline Activity": "Fibrillatory Waves",
        "Ventricular Rate": "Tachycardic",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Not Measurable"
    }
}

ground_truth_key = "Sinus Rhythm"
ground_truth = expected_responses[ground_truth_key]

statistics = defaultdict(lambda: defaultdict(int))
feature_stats = defaultdict(lambda: defaultdict(list))
precision_scores = defaultdict(list)
recall_scores = defaultdict(list)
f1_scores = defaultdict(list)
error_rates = defaultdict(float)
confidence_intervals = defaultdict(lambda: (0, 0))

# Helper function to calculate confidence intervals
def calculate_confidence_interval(accuracy, total):
    if total > 0:
        z = 1.96  # 95% confidence interval
        p = accuracy / 100
        margin = z * np.sqrt((p * (1 - p)) / total)
        return max(0, p - margin) * 100, min(100, p + margin) * 100
    return 0, 0

def evaluate_response(json_data, ground_truth, features):
    comparison = {}
    for feature in features:
        comparison[feature] = json_data.get(feature, "Missing Value") == ground_truth.get(feature, "Missing Value")
    return comparison

valid_responses = 0
total_responses = 0
results = []

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        total_responses += 1
        file_path = os.path.join(folder_path, filename)

        with open(file_path, 'r') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue

            if isinstance(data, dict) and "Diagnosis" in data:
                valid_responses += 1
                comparison = evaluate_response(data, ground_truth, ground_truth.keys())

                for feature, is_correct in comparison.items():
                    statistics[feature]["correct"] += is_correct
                    statistics[feature]["total"] += 1
                    feature_stats[feature][filename].append(is_correct)

                # Extract prompt name more precisely based on suffix
                prompt = "Prompt 1" if "llama_prompt1" in filename else \
                         "Prompt 2" if "llama_prompt2" in filename else \
                         "Prompt 3" if "llama_prompt3" in filename else "Unknown"

                results.append({
                    "Filename": filename,
                    "Prompt": prompt,
                    **comparison
                })

# Calculate additional metrics
for feature, counts in statistics.items():
    accuracy = counts["correct"] / counts["total"] * 100 if counts["total"] > 0 else 0
    error_rates[feature] = 100 - accuracy
    confidence_intervals[feature] = calculate_confidence_interval(accuracy, counts["total"])
    counts["accuracy"] = accuracy

df = pd.DataFrame(results)
prompt_stats = df.groupby("Prompt").mean(numeric_only=True).reset_index()

# Compute precision, recall, and F1-score for each feature
for feature in ground_truth.keys():
    y_true = [ground_truth[feature]] * len(df)
    y_pred = df[feature].apply(lambda x: ground_truth[feature] if x else "Incorrect").tolist()
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])

    precision_scores[feature].append(precision)
    recall_scores[feature].append(recall)
    f1_scores[feature].append(f1)

# Save results to CSV
output_folder = os.path.join(folder_path, f"{ground_truth_key}_Analysis")
os.makedirs(output_folder, exist_ok=True)

df.to_csv(os.path.join(output_folder, f"detailed_results_{ground_truth_key}.csv"), index=False)
prompt_stats.to_csv(os.path.join(output_folder, f"prompt_summary_{ground_truth_key}.csv"), index=False)

# Visualization
# Feature-level accuracy
features = list(statistics.keys())
accuracies = [statistics[feature]["accuracy"] for feature in features]
error_rates_list = [error_rates[feature] for feature in features]

plt.figure(figsize=(10, 6))
plt.bar(features, accuracies, label="Accuracy (%)", alpha=0.7)
plt.bar(features, error_rates_list, label="Error Rate (%)", alpha=0.7)
plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Feature-Level Accuracy and Error Rate")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "feature_accuracy_error_rate.png"))
plt.show()

# Precision, Recall, and F1-score per feature
precision_vals = [precision_scores[feature][0] * 100 for feature in features]
recall_vals = [recall_scores[feature][0] * 100 for feature in features]
f1_vals = [f1_scores[feature][0] * 100 for feature in features]

plt.figure(figsize=(10, 6))
bar_width = 0.25
x_indices = np.arange(len(features))

plt.bar(x_indices, precision_vals, bar_width, label="Precision (%)")
plt.bar(x_indices + bar_width, recall_vals, bar_width, label="Recall (%)")
plt.bar(x_indices + 2 * bar_width, f1_vals, bar_width, label="F1-Score (%)")

plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Precision, Recall, and F1-Score by Feature")
plt.xticks(x_indices + bar_width, features, rotation=45)
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "precision_recall_f1.png"))
plt.show()

# Print summary
print(f"Analysis for {ground_truth_key} Cases")
print("Feature-level Statistics:")
for feature, counts in statistics.items():
    print(f"{feature}: {counts['accuracy']:.2f}% accuracy, Error Rate: {error_rates[feature]:.2f}%, Confidence Interval: {confidence_intervals[feature]}")

print("\nPrecision, Recall, and F1-Score:")
for feature in ground_truth.keys():
    print(f"{feature}: Precision: {precision_scores[feature][0]:.2f}, Recall: {recall_scores[feature][0]:.2f}, F1-Score: {f1_scores[feature][0]:.2f}")

print("\nPrompt-level Statistics:")
print(prompt_stats)