# -*- coding: utf-8 -*-
"""Copy of qwentest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S1PJlc2XlQqs1bMSycIp6ARscGhgnWsg
"""

!pip install transformers requests

import requests
import json

# Hugging Face endpoint URL
api_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Define the input prompt and image URL
image_url = "https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U"
prompt = f"Describe this image in one sentence."

# Prepare the request payload
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json",
}
payload = {
    "inputs": {
        "text": prompt,
        "image": image_url,
    },
    "parameters": {
        "max_tokens": 150,
        "temperature": 1,
    }
}

# Send the request
response = requests.post(api_url, headers=headers, json=payload)

# Handle the response
if response.status_code == 200:
    result = response.json()
    print("Generated Text:", result.get("generated_text", "No output"))
else:
    print("Error:", response.status_code, response.text)

import base64
import requests

# Convert image to Base64
def image_to_base64(url):
    response = requests.get(url)
    return base64.b64encode(response.content).decode("utf-8")

# Base64 encode the image
image_base64 = image_to_base64("https://drive.google.com/uc?id=1Np3EFGhX9w80ziRETgAQ6l6QaGFSL85m")

# Prepare the payload with Base64 image
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image_base64", "image_data": image_base64},
            {"type": "text", "text": "Describe this image in one sentence."}
        ]
    }
]

payload = {
    "model": "tgi",
    "messages": messages,
    "top_p": None,
    "temperature": 1,
    "max_tokens": 500  # Adjust as needed
}

# Hugging Face endpoint URL and API key
base_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Convert image to Base64
def image_to_base64(url):
    response = requests.get(url)
    return base64.b64encode(response.content).decode("utf-8")

# Base64 encode the image
image_base64 = image_to_base64("https://drive.google.com/uc?id=1Np3EFGhX9w80ziRETgAQ6l6QaGFSL85m")

# Prepare the messages (text and image as separate entries)
messages = [
    {"role": "user", "content": f"Image data (Base64): {image_base64}"},
    {"role": "user", "content": "Describe this image in one sentence."}
]

# Define payload
payload = {
    "model": "tgi",
    "messages": messages,
    "top_p": None,
    "temperature": 1,
    "max_tokens": 500  # Adjust as needed
}

# Define headers
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json",
}

# Make the API request
response = requests.post(base_url + "chat/completions", headers=headers, json=payload)

# Print response
if response.status_code == 200:
    data = response.json()
    print(data)
else:
    print("Error:", response.status_code, response.text)

import requests

# Image URL
image_url = "https://drive.google.com/uc?id=1--3lN2svqZJCojSOffaExW6Sh9PKoljf"

# Hugging Face endpoint URL and API key
base_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Prepare the messages
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image_url", "image_url": {"url": image_url}},
            {"type": "text", "text": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """}
        ]
    }
]

# Define payload
payload = {
    "model": "tgi",
    "messages": messages,
    "top_p": None,
    "temperature": 1,
    "max_tokens": 1000  # Adjusted to fit within 10,001 token limit
}

# Define headers
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json",
}

# Make the API request
response = requests.post(base_url + "chat/completions", headers=headers, json=payload)

# Print response
if response.status_code == 200:
    data = response.json()
    print(data)
else:
    print("Error:", response.status_code, response.text)

import requests
import json
import re

# Image URL
image_url = "https://drive.google.com/uc?id=1--3lN2svqZJCojSOffaExW6Sh9PKoljf"

# Hugging Face endpoint URL and API key
base_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Prepare the messages
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image_url", "image_url": {"url": image_url}},
            {"type": "text", "text": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """}
        ]
    }
]

# Define payload
payload = {
    "model": "tgi",
    "messages": messages,
    "top_p": None,
    "temperature": 1,
    "max_tokens": 2000  # Adjusted to fit within the 10,001 token limit
}

# Define headers
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json",
}

# Make the API request
response = requests.post(base_url + "chat/completions", headers=headers, json=payload)

# Extract and handle the response
if response.status_code == 200:
    try:
        data = response.json()
        content = data.get('choices', [])[0].get('message', {}).get('content', "")

        print("Raw Content Received:")
        print(content)  # Print raw response content for debugging

        # Attempt to extract valid JSON using regex
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            valid_json = json_match.group()
            parsed_json = json.loads(valid_json)
            print("\nParsed JSON:")
            print(json.dumps(parsed_json, indent=4))
        else:
            print("\nError: No valid JSON structure found in the response.")
    except Exception as e:
        print("Error extracting or parsing the response:", e)
else:
    print("Error:", response.status_code, response.text)

import requests
import json
import re

# Image URL
image_url = "https://drive.google.com/uc?id=1--3lN2svqZJCojSOffaExW6Sh9PKoljf"

# Hugging Face endpoint URL and API key
base_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Prepare the messages
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image_url", "image_url": {"url": image_url}},
            {"type": "text", "text": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """}
        ]
    }
]

# Define payload
payload = {
    "model": "tgi",
    "messages": messages,
    "top_p": None,
    "temperature": 1,
    "max_tokens": 1000
}

# Define headers
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json",
}

# Make the API request
response = requests.post(base_url + "chat/completions", headers=headers, json=payload)

# Function to clean JSON response
def clean_json(raw_content):
    try:
        # Extract JSON-like structure with regex
        json_match = re.search(r'\{.*?\}', raw_content, re.DOTALL)
        if json_match:
            valid_json = json_match.group()  # Get the matched JSON portion

            # Load as JSON to validate and parse it
            parsed_json = json.loads(valid_json)

            # Define allowed keys for validation
            allowed_keys = [
                "Diagnosis", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval"
            ]

            # Filter out unwanted keys
            cleaned_json = {key: parsed_json[key] for key in allowed_keys if key in parsed_json}
            return cleaned_json
        else:
            return None
    except Exception as e:
        print("Error cleaning JSON:", e)
        return None

# Extract and handle the response
if response.status_code == 200:
    try:
        data = response.json()
        content = data.get('choices', [])[0].get('message', {}).get('content', "")

        # Clean and extract valid JSON
        cleaned_response = clean_json(content)
        if cleaned_response:
            print(json.dumps(cleaned_response, indent=4))
        else:
            print("No valid JSON structure found in the response.")
    except Exception as e:
        print("Error extracting or parsing the response:", e)
else:
    print("Error:", response.status_code, response.text)

import os
import requests
import json
import re

# Hugging Face endpoint URL and API key
base_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Prompts for analysis
prompts = {
    "prompt1": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.""",
    "prompt2": """Analyze the provided ECG image step by step to determine if it represents an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.""",
    "prompt3": """Evaluate the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Confidence Level": "[High/Medium/Low]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary."""
}

# Folder containing the text file with links
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/testgpt_resized'
links_file = os.path.join(folder_path, 'Public_Image_Links_With_Names.txt')

# Read the image links from the text file
with open(links_file, 'r') as f:
    image_links = [line.strip().split(': ') for line in f.readlines()]

# Function to clean JSON response
def clean_json(raw_content):
    try:
        # Extract JSON-like structure with regex
        json_match = re.search(r'\{.*?\}', raw_content, re.DOTALL)
        if json_match:
            valid_json = json_match.group()
            parsed_json = json.loads(valid_json)

            # Define allowed keys
            allowed_keys = [
                "Diagnosis", "Rhythm Regularity", "P Waves",
                "Baseline Activity", "Ventricular Rate",
                "QRS Complex Morphology", "PR Interval",
                "Confidence Level"
            ]
            # Keep only allowed keys
            cleaned_json = {key: parsed_json.get(key, "Missing Value") for key in allowed_keys if key in parsed_json}
            return cleaned_json
        return None
    except Exception as e:
        print("Error cleaning JSON:", e)
        return None

# Process each image and prompt
for image_name, image_link in image_links:
    for prompt_name, prompt in prompts.items():
        # Prepare the message
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_link}},
                    {"type": "text", "text": prompt}
                ]
            }
        ]

        payload = {
            "model": "tgi",
            "messages": messages,
            "top_p": None,
            "temperature": 1,
            "max_tokens": 1000
        }

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        # Send the API request
        response = requests.post(base_url + "chat/completions", headers=headers, json=payload)

        if response.status_code == 200:
            try:
                data = response.json()
                content = data.get('choices', [])[0].get('message', {}).get('content', "")

                # Clean and save the JSON response
                cleaned_response = clean_json(content)
                if cleaned_response:
                    json_filename = os.path.join(folder_path, f"{image_name}_qwen_{prompt_name}.json")
                    with open(json_filename, 'w') as json_file:
                        json.dump(cleaned_response, json_file, indent=4)
                else:
                    print(f"No valid JSON structure found for {image_name} with {prompt_name}.")
            except Exception as e:
                print(f"Error processing {image_name} with {prompt_name}: {e}")
        else:
            print(f"Error for {image_name} with {prompt_name}: {response.status_code} - {response.text}")

# Remove processed links from the text file
with open(links_file, 'w') as f:
    for image_name, image_link in image_links[1:]:
        f.write(f"{image_name}: {image_link}\n")

"""resize images"""

from PIL import Image
import os
from google.colab import drive

# Mount Google Drive

# Define source and target folders
source_folder = "/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retractednus Rhythm"  # Replace with your source folder path
target_folder = "/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/Sinus Rhythm_resized"  # Replace with your target folder path
os.makedirs(target_folder, exist_ok=True)
# Function to resize an image by percentage
def resize_image_by_percentage(image, reduction_percentage):
    # Calculate new dimensions
    width, height = image.size
    new_width = int(width * (reduction_percentage / 100))
    new_height = int(height * (reduction_percentage / 100))

    # Resize the image while maintaining aspect ratio
    resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    return resized_image

# Function to process all .jpeg files in the source folder
def process_images_by_percentage(reduction_percentage):
    for filename in os.listdir(source_folder):
        # Process only .jpeg files
        if filename.lower().endswith('.jpeg'):
            source_path = os.path.join(source_folder, filename)
            target_path = os.path.join(target_folder, filename)

            # Open the image
            with Image.open(source_path) as img:
                print(f"Original size of {filename}: {img.size}")

                # Resize the image
                resized_img = resize_image_by_percentage(img, reduction_percentage)
                print(f"Resized size of {filename}: {resized_img.size}")

                # Save the resized image to the target folder
                resized_img.save(target_path, format="JPEG", quality=70)  # Lower quality to reduce file size
                print(f"Resized and saved: {target_path}")

                # Check file size
                original_size = os.path.getsize(source_path)
                resized_size = os.path.getsize(target_path)
                print(f"File size reduced from {original_size / 1024:.2f} KB to {resized_size / 1024:.2f} KB\n")

# Set the percentage of reduction (e.g., 50% for half the size)
reduction_percentage = 50

# Process all .jpeg files
process_images_by_percentage(reduction_percentage)

print("Image resizing complete! Resized images are saved in the target folder.")

"""testing on a folder and 3 prompts

afib cases
"""

import os
import requests
import json
import re

# Hugging Face endpoint URL and API key
base_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Prompts for analysis
prompts = {
    "prompt1": """As an experienced cardiologist, analyze the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.""",
    "prompt2": """Analyze the provided ECG image step by step to determine if it represents an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[AFIB/NON-AFIB]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable]",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable]"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.""",
    "prompt3": """Evaluate the provided ECG image to determine if it indicates an AFIB or NON-AFIB case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[Sinus Rhythm/Not Sinus Rhythm]",
        "Confidence Level": "[High/Medium/Low]",
        "Rhythm Regularity": {
            "Observation": "[Regular/Irregularly Irregular/Regularly Irregular]",
            "Confidence": "[High/Medium/Low]"
        },
        "P Waves": {
            "Observation": "[Present/Absent/Indiscernible]",
            "Confidence": "[High/Medium/Low]"
        },
        "Baseline Activity": {
            "Observation": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
            "Confidence": "[High/Medium/Low]"
        },
        "Ventricular Rate": {
            "Observation": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
            "Confidence": "[High/Medium/Low]"
        },
        "QRS Complex Morphology": {
            "Observation": "[Constant/Variable] at [number] milliseconds",
            "Confidence": "[High/Medium/Low]"
        },
        "PR Interval": {
            "Observation": "[Normal/Variable/Prolonged/Short/Not Measurable] at [number] milliseconds",
            "Confidence": "[High/Medium/Low]"
        }
    }

    Ensure the JSON output is well-formed and contains no additional commentary."""
}

# Folder containing the text file with links
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/A.Fib_resized'
links_file = os.path.join(folder_path, 'Public_Image_Links_With_Names.txt')

# Read the image links from the text file
with open(links_file, 'r') as f:
    image_links = [line.strip().split(': ') for line in f.readlines()]

# Function to clean JSON response
def clean_json(raw_content, prompt_name):
    try:
        # Extract JSON-like structure with regex
        json_match = re.search(r'\{.*?\}', raw_content, re.DOTALL)
        if json_match:
            valid_json = json_match.group()
            parsed_json = json.loads(valid_json)

            # Define allowed keys
            if prompt_name == "prompt3":
                allowed_keys = [
                    "Diagnosis", "Confidence Level",
                    "Rhythm Regularity", "P Waves",
                    "Baseline Activity", "Ventricular Rate",
                    "QRS Complex Morphology", "PR Interval"
                ]
            else:
                allowed_keys = [
                    "Diagnosis", "Rhythm Regularity", "P Waves",
                    "Baseline Activity", "Ventricular Rate",
                    "QRS Complex Morphology", "PR Interval"
                ]

            # Filter keys and retain nested structures for Prompt 3
            if prompt_name == "prompt3":
                cleaned_json = {}
                for key in allowed_keys:
                    if isinstance(parsed_json.get(key), dict):
                        cleaned_json[key] = {
                            "Observation": parsed_json[key].get("Observation", "Missing Value"),
                            "Confidence": parsed_json[key].get("Confidence", "Missing Value"),
                        }
                    else:
                        cleaned_json[key] = parsed_json.get(key, "Missing Value")
                return cleaned_json
            else:
                return {key: parsed_json[key] for key in allowed_keys if key in parsed_json}
        return None
    except Exception as e:
        print("Error cleaning JSON:", e)
        return None

# Process each image and prompt
for image_name, image_link in image_links:
    for prompt_name, prompt in prompts.items():
        # Prepare the message
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_link}},
                    {"type": "text", "text": prompt}
                ]
            }
        ]

        payload = {
            "model": "tgi",
            "messages": messages,
            "top_p": None,
            "temperature": 1,
            "max_tokens": 1000
        }

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        # Send the API request
        response = requests.post(base_url + "chat/completions", headers=headers, json=payload)

        if response.status_code == 200:
            try:
                data = response.json()
                content = data.get('choices', [])[0].get('message', {}).get('content', "")

                # Clean and save the JSON response
                cleaned_response = clean_json(content, prompt_name)
                if cleaned_response:
                    json_filename = os.path.join(folder_path, f"{image_name}_qwen_{prompt_name}.json")
                    with open(json_filename, 'w') as json_file:
                        json.dump(cleaned_response, json_file, indent=4)
                else:
                    print(f"No valid JSON structure found for {image_name} with {prompt_name}.")
            except Exception as e:
                print(f"Error processing {image_name} with {prompt_name}: {e}")
        else:
            print(f"Error for {image_name} with {prompt_name}: {response.status_code} - {response.text}")

# Remove processed links from the text file
with open(links_file, 'w') as f:
    for image_name, image_link in image_links[1:]:
        f.write(f"{image_name}: {image_link}\n")

import os
import requests
import json
import re

# Hugging Face endpoint URL and API key
base_url = "https://nx0zpb4x6wqcjfrd.us-east-1.aws.endpoints.huggingface.cloud/v1/"
api_key = ""

# Prompts for analysis
prompts = {
   "prompt1": """ As an experienced cardiologist, analyze the provided ECG image to determine if it indicates a Sinus Rhythm or Not Sinus Rhythm case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[Sinus Rhythm/Not Sinus Rhythm]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable] at [number] milliseconds",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable] at [number] milliseconds"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt2": """Analyze the provided ECG image step by step to determine if it represents a a Sinus Rhythm or Not Sinus Rhythm case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[Sinus Rhythm/Not Sinus Rhythm]",
        "Rhythm Regularity": "[Regular/Irregularly Irregular/Regularly Irregular]",
        "P Waves": "[Present/Absent/Indiscernible]",
        "Baseline Activity": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
        "Ventricular Rate": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
        "QRS Complex Morphology": "[Constant/Variable] at [number] milliseconds",
        "PR Interval": "[Normal/Variable/Prolonged/Short/Not Measurable] at [number] milliseconds"
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """,

    "prompt3": """Evaluate the provided ECG image to determine if it indicates a a Sinus Rhythm or Not Sinus Rhythm case. Respond strictly in JSON format, without additional explanations or comments. The response must follow this structure:

    {
        "Diagnosis": "[Sinus Rhythm/Not Sinus Rhythm]",
        "Confidence Level": "[High/Medium/Low]",
        "Rhythm Regularity": {
            "Observation": "[Regular/Irregularly Irregular/Regularly Irregular]",
            "Confidence": "[High/Medium/Low]"
        },
        "P Waves": {
            "Observation": "[Present/Absent/Indiscernible]",
            "Confidence": "[High/Medium/Low]"
        },
        "Baseline Activity": {
            "Observation": "[Normal/Chaotic/Fibrillatory Waves/Sawtooth Flutter Waves]",
            "Confidence": "[High/Medium/Low]"
        },
        "Ventricular Rate": {
            "Observation": "[Normal/Bradycardic/Tachycardic] at [specific rate] bpm",
            "Confidence": "[High/Medium/Low]"
        },
        "QRS Complex Morphology": {
            "Observation": "[Constant/Variable] at [number] milliseconds",
            "Confidence": "[High/Medium/Low]"
        },
        "PR Interval": {
            "Observation": "[Normal/Variable/Prolonged/Short/Not Measurable] at [number] milliseconds",
            "Confidence": "[High/Medium/Low]"
        }
    }

    Ensure the JSON output is well-formed and contains no additional commentary.
    """
}

# Folder containing the text file with links
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/Sinus Rhythm_resized'
links_file = os.path.join(folder_path, 'Public_Image_Links_With_Names.txt')

# Read the image links from the text file
with open(links_file, 'r') as f:
    image_links = [line.strip().split(': ') for line in f.readlines()]

# Function to clean JSON response
def clean_json(raw_content, prompt_name):
    try:
        # Extract JSON-like structure with regex
        json_match = re.search(r'\{.*?\}', raw_content, re.DOTALL)
        if json_match:
            valid_json = json_match.group()
            parsed_json = json.loads(valid_json)

            # Define allowed keys
            if prompt_name == "prompt3":
                allowed_keys = [
                    "Diagnosis", "Confidence Level",
                    "Rhythm Regularity", "P Waves",
                    "Baseline Activity", "Ventricular Rate",
                    "QRS Complex Morphology", "PR Interval"
                ]
            else:
                allowed_keys = [
                    "Diagnosis", "Rhythm Regularity", "P Waves",
                    "Baseline Activity", "Ventricular Rate",
                    "QRS Complex Morphology", "PR Interval"
                ]

            # Filter keys and retain nested structures for Prompt 3
            if prompt_name == "prompt3":
                cleaned_json = {}
                for key in allowed_keys:
                    if isinstance(parsed_json.get(key), dict):
                        cleaned_json[key] = {
                            "Observation": parsed_json[key].get("Observation", "Missing Value"),
                            "Confidence": parsed_json[key].get("Confidence", "Missing Value"),
                        }
                    else:
                        cleaned_json[key] = parsed_json.get(key, "Missing Value")
                return cleaned_json
            else:
                return {key: parsed_json[key] for key in allowed_keys if key in parsed_json}
        return None
    except Exception as e:
        print("Error cleaning JSON:", e)
        return None

# Process each image and prompt
for image_name, image_link in image_links:
    for prompt_name, prompt in prompts.items():
        # Prepare the message
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_link}},
                    {"type": "text", "text": prompt}
                ]
            }
        ]

        payload = {
            "model": "tgi",
            "messages": messages,
            "top_p": None,
            "temperature": 1,
            "max_tokens": 1000
        }

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        # Send the API request
        response = requests.post(base_url + "chat/completions", headers=headers, json=payload)

        if response.status_code == 200:
            try:
                data = response.json()
                content = data.get('choices', [])[0].get('message', {}).get('content', "")

                # Clean and save the JSON response
                cleaned_response = clean_json(content, prompt_name)
                if cleaned_response:
                    json_filename = os.path.join(folder_path, f"{image_name}_qwen_{prompt_name}.json")
                    with open(json_filename, 'w') as json_file:
                        json.dump(cleaned_response, json_file, indent=4)
                else:
                    print(f"No valid JSON structure found for {image_name} with {prompt_name}.")
            except Exception as e:
                print(f"Error processing {image_name} with {prompt_name}: {e}")
        else:
            print(f"Error for {image_name} with {prompt_name}: {response.status_code} - {response.text}")

# Remove processed links from the text file
with open(links_file, 'w') as f:
    for image_name, image_link in image_links[1:]:
        f.write(f"{image_name}: {image_link}\n")

import os
import json
from collections import defaultdict
import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Update the path to the folder containing the JSON files
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/Sinus Rhythm_resized'

# Ground truth definitions for Sinus Rhythm and AFIB
expected_responses = {
    "Sinus Rhythm": {
        "Diagnosis": "Sinus Rhythm",
        "Rhythm Regularity": "Regular",
        "P Waves": "Present",
        "Baseline Activity": "Normal",
        "Ventricular Rate": "Normal",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Normal"
    },
    "AFIB": {
        "Diagnosis": "AFIB",
        "Rhythm Regularity": "Irregularly Irregular",
        "P Waves": "Absent",
        "Baseline Activity": "Fibrillatory Waves",
        "Ventricular Rate": "Tachycardic",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Not Measurable"
    }
}

ground_truth_key = "Sinus Rhythm"
ground_truth = expected_responses[ground_truth_key]

statistics = defaultdict(lambda: defaultdict(int))
feature_stats = defaultdict(lambda: defaultdict(list))
missing_prompts = defaultdict(int)
precision_scores = defaultdict(list)
recall_scores = defaultdict(list)
f1_scores = defaultdict(list)
error_rates = defaultdict(float)
confidence_intervals = defaultdict(lambda: (0, 0))

# Helper function to calculate confidence intervals
def calculate_confidence_interval(accuracy, total):
    if total > 0:
        z = 1.96  # 95% confidence interval
        p = accuracy / 100
        margin = z * np.sqrt((p * (1 - p)) / total)
        return max(0, p - margin) * 100, min(100, p + margin) * 100
    return 0, 0

def evaluate_response(json_data, ground_truth, features):
    comparison = {}
    for feature in features:
        comparison[feature] = json_data.get(feature, "Missing Value") == ground_truth.get(feature, "Missing Value")
    return comparison

valid_responses = 0
total_responses = 0
results = []

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        total_responses += 1
        file_path = os.path.join(folder_path, filename)

        with open(file_path, 'r') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue

            # Check for missing or irrelevant descriptions for prompts
            if "Diagnosis" not in data:
                # Document missing prompt
                prompt = "Prompt 1" if "qwen_prompt1" in filename else \
                         "Prompt 2" if "qwen_prompt2" in filename else \
                         "Prompt 3" if "qwen_prompt3" in filename else "Unknown"
                missing_prompts[prompt] += 1
                continue

            valid_responses += 1
            comparison = evaluate_response(data, ground_truth, ground_truth.keys())

            for feature, is_correct in comparison.items():
                statistics[feature]["correct"] += is_correct
                statistics[feature]["total"] += 1
                feature_stats[feature][filename].append(is_correct)

            # Extract prompt name more precisely based on suffix
            prompt = "Prompt 1" if "qwen_prompt1" in filename else \
                     "Prompt 2" if "qwen_prompt2" in filename else \
                     "Prompt 3" if "qwen_prompt3" in filename else "Unknown"

            results.append({
                "Filename": filename,
                "Prompt": prompt,
                **comparison
            })

# Calculate additional metrics
for feature, counts in statistics.items():
    accuracy = counts["correct"] / counts["total"] * 100 if counts["total"] > 0 else 0
    error_rates[feature] = 100 - accuracy
    confidence_intervals[feature] = calculate_confidence_interval(accuracy, counts["total"])
    counts["accuracy"] = accuracy

df = pd.DataFrame(results)
prompt_stats = df.groupby("Prompt").mean(numeric_only=True).reset_index()

# Compute precision, recall, and F1-score for each feature
for feature in ground_truth.keys():
    y_true = [ground_truth[feature]] * len(df)
    y_pred = df[feature].apply(lambda x: ground_truth[feature] if x else "Incorrect").tolist()
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])

    precision_scores[feature].append(precision)
    recall_scores[feature].append(recall)
    f1_scores[feature].append(f1)

# Save results to CSV
output_folder = os.path.join(folder_path, f"{ground_truth_key}_Analysis")
os.makedirs(output_folder, exist_ok=True)

df.to_csv(os.path.join(output_folder, f"detailed_results_{ground_truth_key}.csv"), index=False)
prompt_stats.to_csv(os.path.join(output_folder, f"prompt_summary_{ground_truth_key}.csv"), index=False)

# Save missing prompts summary
missing_prompts_df = pd.DataFrame(list(missing_prompts.items()), columns=["Prompt", "Missing Count"])
missing_prompts_df.to_csv(os.path.join(output_folder, "missing_prompts_summary.csv"), index=False)

# Visualization
# Feature-level accuracy
features = list(statistics.keys())
accuracies = [statistics[feature]["accuracy"] for feature in features]
error_rates_list = [error_rates[feature] for feature in features]

plt.figure(figsize=(10, 6))
plt.bar(features, accuracies, label="Accuracy (%)", alpha=0.7)
plt.bar(features, error_rates_list, label="Error Rate (%)", alpha=0.7)
plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Feature-Level Accuracy and Error Rate")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "feature_accuracy_error_rate.png"))
plt.show()

# Precision, Recall, and F1-score per feature
precision_vals = [precision_scores[feature][0] * 100 for feature in features]
recall_vals = [recall_scores[feature][0] * 100 for feature in features]
f1_vals = [f1_scores[feature][0] * 100 for feature in features]

plt.figure(figsize=(10, 6))
bar_width = 0.25
x_indices = np.arange(len(features))

plt.bar(x_indices, precision_vals, bar_width, label="Precision (%)")
plt.bar(x_indices + bar_width, recall_vals, bar_width, label="Recall (%)")
plt.bar(x_indices + 2 * bar_width, f1_vals, bar_width, label="F1-Score (%)")

plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Precision, Recall, and F1-Score by Feature")
plt.xticks(x_indices + bar_width, features, rotation=45)
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "precision_recall_f1.png"))
plt.show()

# Print summary
print(f"Analysis for {ground_truth_key} Cases")
print("Feature-level Statistics:")
for feature, counts in statistics.items():
    print(f"{feature}: {counts['accuracy']:.2f}% accuracy, Error Rate: {error_rates[feature]:.2f}%, Confidence Interval: {confidence_intervals[feature]}")

print("\nPrecision, Recall, and F1-Score:")
for feature in ground_truth.keys():
    print(f"{feature}: Precision: {precision_scores[feature][0]:.2f}, Recall: {recall_scores[feature][0]:.2f}, F1-Score: {f1_scores[feature][0]:.2f}")

print("\nPrompt-level Statistics:")
print(prompt_stats)

print("\nMissing Prompts Summary:")
print(missing_prompts_df)

import os
import json
from collections import defaultdict
import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Update the path to the folder containing the JSON files
folder_path = '/content/drive/MyDrive/aub/fall 2024-2025/vip ai in med/testing llms for afib reasoning/V2-dataset-retracted/A.Fib_resized'

# Ground truth definitions for Sinus Rhythm and AFIB
expected_responses = {
    "Sinus Rhythm": {
        "Diagnosis": "Sinus Rhythm",
        "Rhythm Regularity": "Regular",
        "P Waves": "Present",
        "Baseline Activity": "Normal",
        "Ventricular Rate": "Normal",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Normal"
    },
    "AFIB": {
        "Diagnosis": "AFIB",
        "Rhythm Regularity": "Irregularly Irregular",
        "P Waves": "Absent",
        "Baseline Activity": "Fibrillatory Waves",
        "Ventricular Rate": "Tachycardic",
        "QRS Complex Morphology": "Constant",
        "PR Interval": "Not Measurable"
    }
}

ground_truth_key = "AFIB"
ground_truth = expected_responses[ground_truth_key]

statistics = defaultdict(lambda: defaultdict(int))
feature_stats = defaultdict(lambda: defaultdict(list))
missing_prompts = defaultdict(int)
precision_scores = defaultdict(list)
recall_scores = defaultdict(list)
f1_scores = defaultdict(list)
error_rates = defaultdict(float)
confidence_intervals = defaultdict(lambda: (0, 0))

# Helper function to calculate confidence intervals
def calculate_confidence_interval(accuracy, total):
    if total > 0:
        z = 1.96  # 95% confidence interval
        p = accuracy / 100
        margin = z * np.sqrt((p * (1 - p)) / total)
        return max(0, p - margin) * 100, min(100, p + margin) * 100
    return 0, 0

def evaluate_response(json_data, ground_truth, features):
    comparison = {}
    for feature in features:
        comparison[feature] = json_data.get(feature, "Missing Value") == ground_truth.get(feature, "Missing Value")
    return comparison

valid_responses = 0
total_responses = 0
results = []

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        total_responses += 1
        file_path = os.path.join(folder_path, filename)

        with open(file_path, 'r') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue

            # Check for missing or irrelevant descriptions for prompts
            if "Diagnosis" not in data:
                # Document missing prompt
                prompt = "Prompt 1" if "qwen_prompt1" in filename else \
                         "Prompt 2" if "qwen_prompt2" in filename else \
                         "Prompt 3" if "qwen_prompt3" in filename else "Unknown"
                missing_prompts[prompt] += 1
                continue

            valid_responses += 1
            comparison = evaluate_response(data, ground_truth, ground_truth.keys())

            for feature, is_correct in comparison.items():
                statistics[feature]["correct"] += is_correct
                statistics[feature]["total"] += 1
                feature_stats[feature][filename].append(is_correct)

            # Extract prompt name more precisely based on suffix
            prompt = "Prompt 1" if "qwen_prompt1" in filename else \
                     "Prompt 2" if "qwen_prompt2" in filename else \
                     "Prompt 3" if "qwen_prompt3" in filename else "Unknown"

            results.append({
                "Filename": filename,
                "Prompt": prompt,
                **comparison
            })

# Calculate additional metrics
for feature, counts in statistics.items():
    accuracy = counts["correct"] / counts["total"] * 100 if counts["total"] > 0 else 0
    error_rates[feature] = 100 - accuracy
    confidence_intervals[feature] = calculate_confidence_interval(accuracy, counts["total"])
    counts["accuracy"] = accuracy

df = pd.DataFrame(results)
prompt_stats = df.groupby("Prompt").mean(numeric_only=True).reset_index()

# Compute precision, recall, and F1-score for each feature
for feature in ground_truth.keys():
    y_true = [ground_truth[feature]] * len(df)
    y_pred = df[feature].apply(lambda x: ground_truth[feature] if x else "Incorrect").tolist()
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0, pos_label=ground_truth[feature])

    precision_scores[feature].append(precision)
    recall_scores[feature].append(recall)
    f1_scores[feature].append(f1)

# Save results to CSV
output_folder = os.path.join(folder_path, f"{ground_truth_key}_Analysis")
os.makedirs(output_folder, exist_ok=True)

df.to_csv(os.path.join(output_folder, f"detailed_results_{ground_truth_key}.csv"), index=False)
prompt_stats.to_csv(os.path.join(output_folder, f"prompt_summary_{ground_truth_key}.csv"), index=False)

# Save missing prompts summary
missing_prompts_df = pd.DataFrame(list(missing_prompts.items()), columns=["Prompt", "Missing Count"])
missing_prompts_df.to_csv(os.path.join(output_folder, "missing_prompts_summary.csv"), index=False)

# Visualization
# Feature-level accuracy
features = list(statistics.keys())
accuracies = [statistics[feature]["accuracy"] for feature in features]
error_rates_list = [error_rates[feature] for feature in features]

plt.figure(figsize=(10, 6))
plt.bar(features, accuracies, label="Accuracy (%)", alpha=0.7)
plt.bar(features, error_rates_list, label="Error Rate (%)", alpha=0.7)
plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Feature-Level Accuracy and Error Rate")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "feature_accuracy_error_rate.png"))
plt.show()

# Precision, Recall, and F1-score per feature
precision_vals = [precision_scores[feature][0] * 100 for feature in features]
recall_vals = [recall_scores[feature][0] * 100 for feature in features]
f1_vals = [f1_scores[feature][0] * 100 for feature in features]

plt.figure(figsize=(10, 6))
bar_width = 0.25
x_indices = np.arange(len(features))

plt.bar(x_indices, precision_vals, bar_width, label="Precision (%)")
plt.bar(x_indices + bar_width, recall_vals, bar_width, label="Recall (%)")
plt.bar(x_indices + 2 * bar_width, f1_vals, bar_width, label="F1-Score (%)")

plt.xlabel("Features")
plt.ylabel("Percentage (%)")
plt.title("Precision, Recall, and F1-Score by Feature")
plt.xticks(x_indices + bar_width, features, rotation=45)
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "precision_recall_f1.png"))
plt.show()

# Print summary
print(f"Analysis for {ground_truth_key} Cases")
print("Feature-level Statistics:")
for feature, counts in statistics.items():
    print(f"{feature}: {counts['accuracy']:.2f}% accuracy, Error Rate: {error_rates[feature]:.2f}%, Confidence Interval: {confidence_intervals[feature]}")

print("\nPrecision, Recall, and F1-Score:")
for feature in ground_truth.keys():
    print(f"{feature}: Precision: {precision_scores[feature][0]:.2f}, Recall: {recall_scores[feature][0]:.2f}, F1-Score: {f1_scores[feature][0]:.2f}")

print("\nPrompt-level Statistics:")
print(prompt_stats)

print("\nMissing Prompts Summary:")
print(missing_prompts_df)